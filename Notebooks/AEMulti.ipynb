{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dbd1abc",
   "metadata": {
    "cellId": "nfdlo1quz8scjxvvzbgk2c",
    "execution_id": "8158e8a3-0187-4995-bb83-e15f8e0239ca"
   },
   "source": [
    "# TOM pattern recognition : Deep Time-Series Clustering\n",
    "\n",
    "```\n",
    "Author: Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "```\n",
    "The notebook contains the following main sections : \n",
    "  1. Retrieve the data\n",
    "  2. Set subsequence size m\n",
    "  3. Define Auto encoder model \n",
    "  4. Train model \n",
    "\n",
    "Main libraries used :     \n",
    "- torch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5ce65",
   "metadata": {
    "cellId": "9yh56qfa03v5vq90o7twj",
    "execution_id": "ffffebda-e4e0-48e1-8c06-8dd9c32fd258"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6668418b",
   "metadata": {
    "cellId": "950d5l1dfgz0exaw2cef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device : cpu\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Training device : {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f17045e",
   "metadata": {
    "cellId": "tkxd644h2rmsgprcx9wi8f"
   },
   "outputs": [],
   "source": [
    "from utils import get_data\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1bb635",
   "metadata": {
    "cellId": "tgvrxf3ppelv4njawu0ej",
    "execution_id": "da46ad0d-15a6-4a7a-b5ac-b18b42973667"
   },
   "source": [
    "## Retrieve dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6629eb2f",
   "metadata": {
    "cellId": "njwi80hoszoccc6shjlvaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total filtered repos : 464\n",
      "Max commits in data : 1838\n"
     ]
    }
   ],
   "source": [
    "all_data = get_data(target_metrics=['total_removed', 'total_added', 'total_changed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7edba2f",
   "metadata": {
    "cellId": "d8ud8n89wtov6vhxt1fk6q",
    "execution_id": "20105792-c2f7-4565-944a-40d159cb613b"
   },
   "source": [
    "## Create Subsequences & Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1a6cbd8",
   "metadata": {
    "cellId": "utq57z6uxm5yk90dq4k09"
   },
   "outputs": [],
   "source": [
    "def calc_seq_stats(T, t_stamps=None):\n",
    "    # get timestamp stats\n",
    "    sorted_t_stamps_diff = np.diff(np.sort(t_stamps))/np.timedelta64(1, 'h')\n",
    "    tstamps_min, tstamps_max, tstamps_mode = sorted_t_stamps_diff.min(), sorted_t_stamps_diff.max(), stats.mode(sorted_t_stamps_diff)[0][0]\n",
    "    tstamps_mean, tstamps_std = sorted_t_stamps_diff.mean(), sorted_t_stamps_diff.std()\n",
    "    \n",
    "    return np.array([np.mean(T), np.std(T), stats.mode(T)[0][0], np.min(T), np.max(T),\n",
    "                     tstamps_min, tstamps_max, tstamps_mode, tstamps_mean, tstamps_std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fae84d2",
   "metadata": {
    "cellId": "fry2qehflx7m3fv4fzx74q"
   },
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "\n",
    "def generate_subsequences(all_data, window_size = 10):\n",
    "    X = []\n",
    "    Stats = []\n",
    "    indx_repo_map = {}\n",
    "    i = 0\n",
    "    for key, val in all_data.items():\n",
    "        temp = np.stack([val[metric] for metric in ['total_removed', 'total_added', 'total_changed']])\n",
    "        tstamp = val.get('time_stamps')\n",
    "        start = 1\n",
    "        end = start + window_size\n",
    "        while temp.shape[1] > end :\n",
    "            X.append(temp[:,start:end])\n",
    "            res = [calc_seq_stats(temp[j,start:end], tstamp[start:end]) for j in range(temp.shape[0])]\n",
    "            Stats.append(res)\n",
    "            start += 1\n",
    "            end += 1\n",
    "            indx_repo_map[i] = key\n",
    "            i += 1\n",
    "    \n",
    "    return np.array(X), indx_repo_map, Stats\n",
    "\n",
    "xtrain, indx_repo_map, S = generate_subsequences(all_data, window_size=window_size)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "xtrain_std = scaler.fit_transform(xtrain.reshape(-1, window_size)).reshape(-1,3, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71478ef0",
   "metadata": {
    "cellId": "pudavwxhf3vf1ntkgxb2",
    "execution_id": "64129ae2-aa5a-492d-887b-def50f7206e5"
   },
   "source": [
    "## Create Tensor Dataset and Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "557a1a0a",
   "metadata": {
    "cellId": "x9r9l71835yb3jfy6yy4o"
   },
   "outputs": [],
   "source": [
    "batch_size = 128 \n",
    "x_tensor = torch.from_numpy(xtrain_std).float()\n",
    "\n",
    "# Create dataset and Data Loader\n",
    "dataset = torch.utils.data.TensorDataset(x_tensor,x_tensor)\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3f4ea",
   "metadata": {
    "cellId": "aedqyid4o9vxxezqrzltmq",
    "execution_id": "81a78996-c40e-4fd3-bfcf-21dc9c631251"
   },
   "source": [
    "## Define model Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69992771",
   "metadata": {
    "cellId": "1a0q3w668vspmdnn0jyi4o"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class Simple1DModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simple1DModel, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(torch.nn.Conv1d(in_channels=3, out_channels=3, kernel_size=3, stride=1), \n",
    "                                           torch.nn.Tanh(),\n",
    "                                           torch.nn.Flatten(),\n",
    "                                           torch.nn.Linear(24, 10)\n",
    "                                          )\n",
    "        self.decoder = torch.nn.Sequential(torch.nn.Tanh(),\n",
    "                                           torch.nn.ConvTranspose1d(in_channels=1, out_channels=3, kernel_size=3, stride=1),\n",
    "                                           torch.nn.Tanh(),\n",
    "                                           #torch.nn.BatchNorm1d(3),\n",
    "                                           torch.nn.Conv1d(in_channels=3, out_channels=3, kernel_size=3, stride=1)\n",
    "                                          )\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        x = encoded.unsqueeze(1)\n",
    "        decoded = self.decoder(x)\n",
    "        \n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f52dba",
   "metadata": {
    "cellId": "bep6m097c9jmyo4nrnvf3",
    "execution_id": "c9d3f77c-a69c-48a4-9539-0d4ab1487a8d"
   },
   "source": [
    "## Unsupervised Data-driven Automotive Diagnostics with Improved Deep Temporal Clustering\n",
    "\n",
    "DeepCluster: A Deep Convolutional Auto-encoder with Embedded Clustering\n",
    "\n",
    "The proposed approach embeds clustering algorithm Kmeans into a DCAE framework which is jointly optimized and trained in a fully unsupervised manner. The methods alternately learn effective feature representation and cluster assignment through DCAE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "991b105d",
   "metadata": {
    "cellId": "f46sfpglleawu1ym2jap1g"
   },
   "outputs": [],
   "source": [
    "class DCAE_Kmeans(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DCAE_Kmeans, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(torch.nn.Conv1d(in_channels=3, out_channels=15, kernel_size=3, stride=1, padding='same'), \n",
    "                                           torch.nn.LeakyReLU(),\n",
    "                                           torch.nn.MaxPool1d(kernel_size=2),\n",
    "                                           torch.nn.Conv1d(in_channels=15, out_channels=1, kernel_size=3, stride=1, padding='same'),\n",
    "                                           torch.nn.LeakyReLU(),\n",
    "                                           torch.nn.MaxPool1d(kernel_size=2)\n",
    "                                          )\n",
    "        self.decoder = torch.nn.Sequential(torch.nn.Upsample(scale_factor=2),\n",
    "                                           torch.nn.Conv1d(in_channels=1, out_channels=15, kernel_size=3, stride=1, padding='same'),\n",
    "                                           torch.nn.LeakyReLU(),\n",
    "                                           torch.nn.Upsample(scale_factor=2.5),\n",
    "                                           torch.nn.Conv1d(in_channels=15, out_channels=3, kernel_size=3, stride=1, padding='same'),\n",
    "                                           torch.nn.LeakyReLU()\n",
    "                                          )\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        return decoded, encoded.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-panel",
   "metadata": {},
   "source": [
    "## DCAE - Kmeans Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "annual-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def clustering_loss(latent_represntation, Lambda=0.3):\n",
    "    loss = KMeans().fit_transform(latent_represntation).sum()\n",
    "    \n",
    "    return Lambda * np.mean(loss**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "better-american",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = DCAE_Kmeans().to(device)\n",
    "\n",
    "DAEcriterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "agreed-approval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 , Loss : 0.0072275710\n",
      "Epoch : 2 , Loss : 0.0008240599\n",
      "Epoch : 3 , Loss : 0.0012509177\n",
      "Epoch : 4 , Loss : 0.0011560170\n",
      "Epoch : 5 , Loss : 0.0010756677\n",
      "Epoch : 6 , Loss : 0.0020573153\n",
      "Epoch : 7 , Loss : 0.0023036032\n",
      "Epoch : 8 , Loss : 0.0020752278\n",
      "Epoch : 9 , Loss : 0.0025694313\n",
      "Epoch : 10 , Loss : 0.0029757300\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "train_loss_hist = []\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    for i, (data, label) in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data.to(device), label.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        outputs, enc_data = model(inputs)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        DAEloss = DAEcriterion(outputs, labels)\n",
    "        kmensloss = clustering_loss(enc_data.cpu().detach().numpy())\n",
    "        \n",
    "        loss = DAEloss + kmensloss\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        #if i % 100 == 0:\n",
    "            #print(f'Batch : {i+1} , Loss : {running_loss:.10f}')\n",
    "    print(f'Epoch : {epoch} , Loss : {running_loss/len(trainloader):.10f}')\n",
    "    train_loss_hist.append(running_loss/len(trainloader))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e77ee1",
   "metadata": {
    "cellId": "yh83u3ryekl8t8rwtqn0o",
    "execution_id": "2b75bd48-0912-4866-8f96-99dfad591737"
   },
   "source": [
    "## Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ab788",
   "metadata": {
    "cellId": "sier1bnlxsk2t7im5do213"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "n_epochs = 10\n",
    "model = Simple1DModel().to(device)\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045fd988",
   "metadata": {
    "cellId": "1u94qlcn1qyy07stkr6xi",
    "execution_id": "e0701112-6217-4924-9d99-11913d5e5fc6"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acee5850",
   "metadata": {
    "cellId": "fajo4gul57f07vmmz3sujhd"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e8a9f",
   "metadata": {
    "cellId": "6dn71873bxlmmdwzoddmps"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model.train()\n",
    "train_loss_hist = []\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    for i, (data, label) in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data.to(device), label.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print(f'Batch : {i+1} , Loss : {running_loss:.10f}')\n",
    "    print(f'Epoch : {epoch} , Loss : {running_loss/len(trainloader):.10f}')\n",
    "    train_loss_hist.append(running_loss/len(trainloader))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2431ff",
   "metadata": {
    "cellId": "xor9i820wmdieqbjgrgpj",
    "execution_id": "740eb6db-686d-4533-a3aa-ebbd135093f3"
   },
   "source": [
    "## Train Loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8bf5d8",
   "metadata": {
    "cellId": "lvr981hntgjzeddsw21shq"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "fig = px.line(x=np.arange(len(train_loss_hist)), y=train_loss_hist)\n",
    "fig.update_traces(mode=\"markers+lines\")\n",
    "fig.update_layout(title=\"Train loss Plot\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed20c63",
   "metadata": {
    "cellId": "81cml0b7kw7imvd3g4wdze",
    "execution_id": "727dac1f-fb92-4a41-a0eb-8d2e83b47c95"
   },
   "source": [
    "## Cluster the latent space of AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54082b3d",
   "metadata": {
    "cellId": "wy13xrpjy0j7w85156xb79"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "model.eval()\n",
    "encoded_data = model.encoder(x_tensor.to(device)).cpu().detach().numpy()\n",
    "\n",
    "kmeans = KMeans(3)\n",
    "cluster_labels = kmeans.fit_predict(encoded_data)\n",
    "\n",
    "np.unique(cluster_labels,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d40ef2",
   "metadata": {
    "cellId": "xmtfkutk7t354yf5uh6dn"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,10)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(encoded_data)\n",
    "    Sum_of_squared_distances.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7e6b78",
   "metadata": {
    "cellId": "h85kkfnzwv8nx58p3r153f"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "fig = px.line(x=K, y=Sum_of_squared_distances)\n",
    "fig.update_traces(mode=\"markers+lines\")\n",
    "fig.update_layout(title=\"Elbow Method\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f54a7",
   "metadata": {
    "cellId": "tzx5d5p09jyb4x9x63znd"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "Sum_of_squared_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f5ad99",
   "metadata": {
    "cellId": "2mxvcdzlx4q63opvq0bobu"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "notebookId": "e7f65fcb-b54f-4360-9412-8565752587d4",
  "notebookPath": "tom-pattern-recognition/Notebooks/AEMulti.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
